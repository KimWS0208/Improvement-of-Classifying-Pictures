{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import re\n",
    "import csv\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# path_train = \"/home/manmiddle/GPU_WS/128x128_cd/128x128_cd_img_train_26000/\"\n",
    "# path_test = \"/home/manmiddle/GPU_WS/128x128_cd/128x128_cd_img_test_4000/\"\n",
    "# label_csv_train = \"/home/manmiddle/GPU_WS/128x128_cd/26000_label_linux.csv\"\n",
    "# label_csv_test = \"/home/manmiddle/GPU_WS/128x128_cd/4000_label_linux.csv\"\n",
    "\n",
    "#croos validation\n",
    "path_train = \"C:\\\\Users\\\\USER-PC\\\\GPU_KWS\\\\GPU_WS\\\\128x128_cd\\\\128x128_cd_30000\\\\\"\n",
    "\n",
    "label_csv_train = \"C:\\\\Users\\\\USER-PC\\\\GPU_KWS\\\\GPU_WS\\\\128x128_cd\\\\128x128_cd_30000.csv\"\n",
    "\n",
    "\n",
    "# path_train = \"/home/manmiddle/GPU_WS/128x128_cd/128x128_cd_img_train_26000/\"\n",
    "# path_test = \"/home/manmiddle/GPU_Ws/128x128_cd/128x128_cd_img_test_3400/\"\n",
    "# label_csv_train = \"/home/manmiddle/GPU_WS/128x128_cd/26000_label_linux.csv\"\n",
    "# label_csv_test = \"/home/manmiddle/GPU_WS/128x128_cd/128x128_cd_label_test_3400.csv\"\n",
    "\n",
    "random.seed(1234)\n",
    "################################ img_argument\n",
    "def img_rotate(img):\n",
    "    argument = [cv2.ROTATE_180,cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE]\n",
    "    res_narray = np.zeros(shape = img.shape)\n",
    "    for i in range(img.shape[0]):\n",
    "        rotate_val = random.randint(0,2)\n",
    "        res_narray[i] = cv2.rotate(img[i], argument[rotate_val])\n",
    "    # print(res_narray)\n",
    "    return res_narray\n",
    "\n",
    "def img_contrast(color_img):\n",
    "    #print(\"img_contrast_color_img.shape\",color_img.shape)\n",
    "    res_narray = np.zeros(shape = color_img.shape)\n",
    "    #print(\"img_contrast_res_narray.shape_be\",res_narray.shape)\n",
    "    for i in range(color_img.shape[0]):\n",
    "        contrast_val = random.randint(0,50)\n",
    "        plus_img = np.zeros(shape = color_img[i].shape, dtype = np.uint8) + contrast_val\n",
    "        res_narray[i] = cv2.add(color_img[i], plus_img)\n",
    "    #print(\"img_contrast_res_narray.shape_af\",res_narray.shape)\n",
    "    return res_narray\n",
    "\n",
    "def img_flip(img):\n",
    "    res_narray = np.zeros(shape = img.shape)\n",
    "    for i in range(img.shape[0]):\n",
    "        flip_val = random.randint(0,1)\n",
    "        res_narray[i] = cv2.flip(img[i], flip_val)    \n",
    "    return res_narray\n",
    "\n",
    "def img_normal(img):\n",
    "    res_narray = np.zeros(shape = img.shape)\n",
    "    for i in range(img.shape[0]):\n",
    "        res_narray[i] = img[i]\n",
    "    return res_narray\n",
    "\n",
    "############################# 데이터 로드\n",
    "def image_load(path):\n",
    "    file_list = os.listdir(path)\n",
    "    file_name = []\n",
    "    for i in file_list:\n",
    "        a = int(re.sub('[^0-9]','',i))\n",
    "        file_name.append(a)\n",
    "        file_name.sort()\n",
    "    data = []\n",
    "    for i in file_name:\n",
    "        file = path + str(i) + '.jpg'\n",
    "        data.append(file)\n",
    "    image = []\n",
    "    for j in data:\n",
    "        img = cv2.imread(j)\n",
    "        image.append(img)     \n",
    "    return np.array(image),data\n",
    "\n",
    "def label_load(label_csv_test):    \n",
    "    file = list(csv.reader(open(label_csv_test)))\n",
    "    label = np.array(np.squeeze(np.array(file),1).astype(int)-1)\n",
    "    label = np.eye(2)[label]\n",
    "    return label\n",
    "\n",
    "def next_batch(data_list,label,data,idx,batch_size):\n",
    "    batch1 = data_list[idx * batch_size:idx * batch_size + batch_size]\n",
    "    label2 = label[idx * batch_size:idx * batch_size + batch_size]\n",
    "    data3 = data[idx * batch_size:idx * batch_size + batch_size]\n",
    "    index = list(np.arange(len(batch1)))\n",
    "    random.shuffle(index)\n",
    "    index = np.array(index)\n",
    "    data = batch1[index]\n",
    "    label = label2[index]\n",
    "    path = []\n",
    "    for i in index:\n",
    "        path.append(data3[i])\n",
    "    return data, label, path\n",
    "\n",
    "def batch_norm_cnn(batch_image, depth) :\n",
    "    epsilon = 1e-5\n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[depth]), trainable=True)\n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[depth]), trainable=True)\n",
    "    mean, variance = tf.nn.moments(batch_image, axes=[0, 1, 2])\n",
    "    norm_batch = tf.nn.batch_normalization(batch_image, mean, variance, beta, gamma, epsilon)\n",
    "    return norm_batch\n",
    "\n",
    "def batch_norm_flat(batch_flat) :\n",
    "    epsilon = 1e-5\n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[1]), trainable=True)\n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[1]), trainable=True)\n",
    "    mean, variance = tf.nn.moments(batch_flat, axes=[0])\n",
    "    norm_batch = tf.nn.batch_normalization(batch_flat, mean, variance, beta, gamma, epsilon)\n",
    "    return norm_batch\n",
    "\n",
    "def shuffle_batch(data, label, path):\n",
    "    index = np.arange(len(data))\n",
    "    random.shuffle(index)\n",
    "    res_data = data[index]\n",
    "    res_label = label[index]\n",
    "    res_path = []\n",
    "    for i in index:\n",
    "        res_path.append(path[i])\n",
    "    return res_data, res_label, res_path\n",
    "\n",
    "\n",
    "trainX,trainP = image_load(path_train)\n",
    "print(\"trainX=\",trainX.shape, \"\\ntrainP=\",len(trainP))\n",
    "\n",
    "trainY = label_load(label_csv_train)\n",
    "print(\"trainY=\",trainY.shape)\n",
    "\n",
    "# trainX, trainY, trainP = shuffle_batch(trainX, trainY, trainP)\n",
    "\n",
    "trainX1 = trainX[0:int(len(trainX)*0.2)]\n",
    "trainX2 = trainX[int(len(trainX)*0.2):int(len(trainX)*0.4)]\n",
    "trainX3 = trainX[int(len(trainX)*0.4):int(len(trainX)*0.6)]\n",
    "trainX4 = trainX[int(len(trainX)*0.6):int(len(trainX)*0.8)]\n",
    "trainX5 = trainX[int(len(trainX)*0.8):int(len(trainX))]\n",
    "\n",
    "trainX2345 = np.append(np.append(np.append(trainX2,trainX3,axis=0), trainX4, axis=0), trainX5, axis=0)\n",
    "trainX1345 = np.append(np.append(np.append(trainX1,trainX3,axis=0), trainX4, axis=0), trainX5, axis=0)\n",
    "trainX1245 = np.append(np.append(np.append(trainX1,trainX2,axis=0), trainX4, axis=0), trainX5, axis=0)\n",
    "trainX1235 = np.append(np.append(np.append(trainX1,trainX2,axis=0), trainX3, axis=0), trainX5, axis=0)\n",
    "trainX1234= np.append(np.append(np.append(trainX1,trainX2,axis=0), trainX3, axis=0), trainX4, axis=0)\n",
    "\n",
    "# print(\"trainX1=\",trainX1.shape)\n",
    "print(\"trainX1=\",trainX1.shape,\"trainX2=\",trainX2.shape,\"trainX3=\",trainX3.shape,\"trainX4=\",trainX4.shape,\"trainX5=\",trainX5.shape)\n",
    "print(\"trainX1=\",trainX2345.shape,\"trainX2=\",trainX1345.shape,\"trainX3=\",trainX1245.shape,\"trainX4=\",trainX1235.shape,\"trainX5=\",trainX1234.shape)\n",
    "\n",
    "trainP1 = trainP[0:int(len(trainP)*0.2)]\n",
    "trainP2 = trainP[int(len(trainP)*0.2):int(len(trainP)*0.4)]\n",
    "trainP3 = trainP[int(len(trainP)*0.4):int(len(trainP)*0.6)]\n",
    "trainP4 = trainP[int(len(trainP)*0.6):int(len(trainP)*0.8)]\n",
    "trainP5 = trainP[int(len(trainP)*0.8):int(len(trainP))]\n",
    "\n",
    "trainP2345 = np.append(np.append(np.append(trainP2,trainP3,axis=0), trainP4, axis=0), trainP5, axis=0)\n",
    "trainP1345 = np.append(np.append(np.append(trainP1,trainP3,axis=0), trainP4, axis=0), trainP5, axis=0)\n",
    "trainP1245 = np.append(np.append(np.append(trainP1,trainP2,axis=0), trainP4, axis=0), trainP5, axis=0)\n",
    "trainP1235 = np.append(np.append(np.append(trainP1,trainP2,axis=0), trainP3, axis=0), trainP5, axis=0)\n",
    "trainP1234= np.append(np.append(np.append(trainP1,trainP2,axis=0), trainP3, axis=0), trainP4, axis=0)\n",
    "\n",
    "trainY1 = trainY[0:int(len(trainY)*0.2)]\n",
    "trainY2 = trainY[int(len(trainY)*0.2):int(len(trainY)*0.4)]\n",
    "trainY3 = trainY[int(len(trainY)*0.4):int(len(trainY)*0.6)]\n",
    "trainY4 = trainY[int(len(trainY)*0.6):int(len(trainY)*0.8)]\n",
    "trainY5 = trainY[int(len(trainY)*0.8):int(len(trainY))]\n",
    "\n",
    "trainY2345 = np.append(np.append(np.append(trainY2,trainY3,axis=0), trainY4, axis=0), trainY5, axis=0)\n",
    "trainY1345 = np.append(np.append(np.append(trainY1,trainY3,axis=0), trainY4, axis=0), trainY5, axis=0)\n",
    "trainY1245 = np.append(np.append(np.append(trainY1,trainY2,axis=0), trainY4, axis=0), trainY5, axis=0)\n",
    "trainY1235 = np.append(np.append(np.append(trainY1,trainY2,axis=0), trainY3, axis=0), trainY5, axis=0)\n",
    "trainY1234= np.append(np.append(np.append(trainY1,trainY2,axis=0), trainY3, axis=0), trainY4, axis=0)\n",
    "\n",
    "print(\"trainX1=\",trainY1.shape,\"trainX2=\",trainY2.shape,\"trainX3=\",trainY3.shape,\"trainX4=\",trainY4.shape,\"trainX5=\",trainY5.shape)\n",
    "print(\"trainX1=\",trainY2345.shape,\"trainX2=\",trainY1345.shape,\"trainX3=\",trainY1245.shape,\"trainX4=\",trainY1235.shape,\"trainX5=\",trainY1234.shape)\n",
    "\n",
    "\n",
    "selector = tf.placeholder(tf.bool)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "keep_prob2 = tf.placeholder(tf.float32)\n",
    "\n",
    "trainX = trainX2345\n",
    "trainY = trainY2345\n",
    "trainP = trainP2345\n",
    "\n",
    "testX = trainX1\n",
    "testY = trainY1\n",
    "testP = trainP1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "############################### 하이퍼 파라미터\n",
    "lr = 0.0001\n",
    "train_size = len(trainX)\n",
    "print(trainX.shape)\n",
    "print(train_size)\n",
    "test_size = len(testX)\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "total_epoch = 80\n",
    "train_total_batch = int(train_size/batch_size)\n",
    "test_total_batch = int(test_size/batch_size)\n",
    "\n",
    "\n",
    "################################# 가중치 밑 편향,입력\n",
    "x = tf.placeholder(tf.float32, [None, 128, 128, 3])\n",
    "y = tf.placeholder(tf.float32, [None,2])\n",
    "w_1 = tf.get_variable(name='w_1', shape=[8*8*100, 200], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "w_2 = tf.get_variable(name='w_2', shape=[200, 2], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b_1 = tf.Variable(tf.ones(200))\n",
    "b_2 = tf.Variable(tf.ones(2))\n",
    "\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([10, 10, 3, 30], stddev=0.01))\n",
    "w2 = tf.Variable(tf.random_normal([10, 10, 30, 40], stddev=0.01))\n",
    "w3 = tf.Variable(tf.random_normal([7, 7, 40, 50], stddev=0.01))\n",
    "w4 = tf.Variable(tf.random_normal([5, 5, 50, 60], stddev=0.01))\n",
    "w5 = tf.Variable(tf.random_normal([3, 3, 60, 100], stddev=0.01))\n",
    "w6 = tf.Variable(tf.random_normal([3, 3, 100, 100], stddev=0.01))\n",
    "\n",
    "\n",
    "################################ Layer\n",
    "# conv 1층\n",
    "L1 = tf.nn.conv2d(x, w1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.contrib.layers.batch_norm(L1, is_training=selector)\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob2)\n",
    "\n",
    "# conv 2층\n",
    "L2 = tf.nn.conv2d(L1, w2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.contrib.layers.batch_norm(L2, is_training=selector)\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob2)\n",
    "\n",
    "# conv 3층\n",
    "L3 = tf.nn.conv2d(L2, w3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.contrib.layers.batch_norm(L3, is_training=selector)\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, keep_prob2)\n",
    "\n",
    "# conv 4층\n",
    "L4 = tf.nn.conv2d(L3, w4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L4 = tf.contrib.layers.batch_norm(L4, is_training=selector)\n",
    "L4 = tf.nn.relu(L4)\n",
    "L4 = tf.nn.max_pool(L4, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L4 = tf.nn.dropout(L4, keep_prob2)\n",
    "\n",
    "# conv 5층\n",
    "L5 = tf.nn.conv2d(L4, w5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L5 = tf.contrib.layers.batch_norm(L5, is_training=selector)\n",
    "L5 = tf.nn.relu(L5)\n",
    "L5 = tf.nn.dropout(L5, keep_prob2)\n",
    "\n",
    "# conv 6층\n",
    "L6 = tf.nn.conv2d(L5, w6, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L6 = tf.contrib.layers.batch_norm(L6, is_training=selector)\n",
    "L6 = tf.nn.relu(L6)\n",
    "L6 = tf.nn.dropout(L6, keep_prob2)\n",
    "\n",
    "conv_out = tf.reshape(L6, [-1, 8*8*100 ])\n",
    "\n",
    "\n",
    "#hidden 1층\n",
    "result1 = tf.matmul(conv_out,w_1)+b_1\n",
    "batch1 = tf.contrib.layers.batch_norm(result1, is_training=selector)\n",
    "relu1 = tf.nn.relu(batch1)\n",
    "relu1 = tf.nn.dropout(relu1, keep_prob)\n",
    "result_out = tf.matmul(relu1, w_2)+b_2\n",
    "\n",
    "#출력층\n",
    "result_hat = tf.nn.softmax(result_out)\n",
    "result_predict = tf.argmax(result_hat, 1)\n",
    "result_label = tf.argmax(y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(result_predict, result_label),tf.float32))\n",
    "correct_prediction = tf.equal(result_predict, result_label)\n",
    "\n",
    "\n",
    "# ################################### loss 함수, 손실함수, 학습방법 \n",
    "delta = 1e-7\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(result_hat+delta), axis=1)) # 크로스엔트로피 loss값\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr) # 경사하강법\n",
    "\n",
    "# cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=result_out, labels=y))\n",
    "# optimizer =  tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "\n",
    "\n",
    "######################################배치정규화 베타감마 적용하게하는부분\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    # train = optimizer.minimize(loss)\n",
    "    train_step = optimizer.minimize(cross_entropy_loss)\n",
    "\n",
    "saver = tf.train.Saver(save_relative_paths=True)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"train_total_batch = \", train_total_batch)\n",
    "    print(\"test_total_batch = \", test_total_batch)\n",
    "    \n",
    "    #total_wrong = []\n",
    "    wrong_index1_1 = []\n",
    "    # total_wrong_cnt = []\n",
    "    for epoch in range(total_epoch):\n",
    "        train_cost = 0\n",
    "        train_accuracy = 0\n",
    "        test_accuracy = 0\n",
    "        global learningrate\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # cost\n",
    "        for i in range(train_total_batch):\n",
    "            img_func_list = [img_rotate, img_contrast, img_flip, img_normal]\n",
    "            rand = random.randint(0,3)\n",
    "            batch_x_train, batch_y_train, batch_p_train = next_batch(trainX, trainY, trainP,i,batch_size)\n",
    "            _, cost = sess.run([train_step, cross_entropy_loss], feed_dict={x:img_func_list[rand](batch_x_train), y : batch_y_train, selector:True, keep_prob:0.5, keep_prob2:0.5})\n",
    "            train_cost += cost\n",
    "        train_loss_list.append(train_cost/train_total_batch)\n",
    "        \n",
    "        #train\n",
    "        for i in range(train_total_batch):\n",
    "            batch_x_train, batch_y_train ,batch_p_train = next_batch(trainX, trainY, trainP, i, batch_size)\n",
    "            train_acc = sess.run(accuracy , feed_dict={x:batch_x_train, y:batch_y_train, selector:False, keep_prob:1,  keep_prob2:1})\n",
    "            train_accuracy += train_acc\n",
    "        train_acc_list.append(train_accuracy/train_total_batch)\n",
    "        \n",
    "        #test    \n",
    "        for i in range(test_total_batch):\n",
    "            batch_x_test, batch_y_test ,batch_p_test = next_batch(testX, testY, testP, i, batch_size)\n",
    "            test_acc = sess.run(accuracy, feed_dict={x:batch_x_test, y:batch_y_test, selector:False, keep_prob:1,  keep_prob2:1})\n",
    "            test_accuracy += test_acc\n",
    "            \n",
    "             \n",
    "            wrong_test = sess.run(correct_prediction,feed_dict={x:batch_x_test,y:batch_y_test,selector:False,  keep_prob:1,  keep_prob2:1})\n",
    "            for j in range(len(wrong_test)):\n",
    "                if epoch==total_epoch-1:\n",
    "                    if wrong_test[j] == False:\n",
    "                        wrong_index1_1.append(batch_p_test[j]) \n",
    "            #total_wrong.append(wrong_index)\n",
    "        test_acc_list.append(test_accuracy/test_total_batch)\n",
    "        \n",
    "        t1 = time.time()        \n",
    "        total_time = t1-t0\n",
    "        \n",
    "        print('epoch :', epoch + 1,\n",
    "              ' #  total_cost :', round(train_cost/train_total_batch, 3), # 1 에폭의 평균 cost\n",
    "              ' #  train_accuracy :', round(train_accuracy/train_total_batch, 3),\n",
    "              ' #  test_accuracy :', round(test_accuracy/test_total_batch, 3), \n",
    "              ' # 1_epoch_time : ', total_time)\n",
    "\n",
    "x = np.arange(total_epoch)\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
